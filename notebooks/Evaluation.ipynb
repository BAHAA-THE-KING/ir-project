{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc419cb",
   "metadata": {},
   "source": [
    "# Install libs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd0664",
   "metadata": {},
   "source": [
    "## General libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49049a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user ir_datasets\n",
    "!pip install joblib\n",
    "!pip install dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8be70",
   "metadata": {},
   "source": [
    "## Text processing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76abb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6f06d",
   "metadata": {},
   "source": [
    "## TF-IDF libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff764e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a708fb",
   "metadata": {},
   "source": [
    "## Embedding libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb\n",
    "!pip install sentence_transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8aa5ad",
   "metadata": {},
   "source": [
    "## BM25 libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aac8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3ad34",
   "metadata": {},
   "source": [
    "## Plotting libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b8b3cc",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510299e0",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c06689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations\n",
    "DATASETS = {\n",
    "    \"antique\": {\n",
    "        \"name\": \"antique\",\n",
    "        \"description\": \"Question-answer dataset with natural questions from real users\",\n",
    "        \"ir_datasets_id\": \"antique\",\n",
    "        \"ir_datasets_test_id\": \"antique/test/non-offensive\"\n",
    "    },\n",
    "    \"quora\": {\n",
    "        \"name\": \"beir/quora\",\n",
    "        \"description\": \"Quora question pairs dataset from the BEIR benchmark\",\n",
    "        \"ir_datasets_id\": \"beir/quora\",\n",
    "        \"ir_datasets_test_id\": \"beir/quora/dev\"\n",
    "    },\n",
    "    \"webis\": {\n",
    "        \"name\": \"beir/webis-touche2020/v2\",\n",
    "        \"description\": \"Webis TouchÃ© 2020 (v2) dataset from the BEIR benchmark\",\n",
    "        \"ir_datasets_id\": \"beir/webis-touche2020/v2\",\n",
    "        \"ir_datasets_test_id\": \"beir/webis-touche2020/v2\"\n",
    "    },\n",
    "    \"recreation\": {\n",
    "        \"name\": \"lotte/recreation/dev\",\n",
    "        \"description\": \"LOTTE Recreation domain, development split\",\n",
    "        \"ir_datasets_id\": \"lotte/recreation/dev/forum\",\n",
    "        \"ir_datasets_test_id\": \"lotte/recreation/test/forum\"\n",
    "    },\n",
    "    \"wikir\": {\n",
    "        \"name\": \"wikir/en1k\",\n",
    "        \"description\": \"Wiki-Retrieval English 1K dataset\",\n",
    "        \"ir_datasets_id\": \"wikir/en1k\",\n",
    "        \"ir_datasets_test_id\": \"wikir/en1k/test\"\n",
    "    },\n",
    "    \"clinical\": {\n",
    "        \"name\": \"clinicaltrials/2021/trec-ct-2021\",\n",
    "        \"description\": \"ClinicalTrials TREC-CT 2021 dataset\",\n",
    "        \"ir_datasets_id\": \"clinicaltrials/2021/trec-ct-2021\",\n",
    "        \"ir_datasets_test_id\": \"clinicaltrials/2021/trec-ct-2021/test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Default dataset to use if none specified\n",
    "DEFAULT_DATASET = \"antique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449769b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "from typing import TypeAlias\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Doc = namedtuple('Doc', ['doc_id', 'text'])\n",
    "Query = namedtuple('Query', ['query_id', 'text'])\n",
    "Qrel = namedtuple('Qrel', ['query_id', 'doc_id', 'relevance', 'iteration'])\n",
    "\n",
    "def load_dataset(name: str) -> list[Doc]:\n",
    "    dataset = ir_datasets.load(DATASETS[name]['ir_datasets_id'])\n",
    "    \n",
    "    docs = list(dataset.docs_iter())\n",
    "    return docs\n",
    "\n",
    "def load_queries_and_qrels(name: str) -> tuple[list[Query], list[Qrel]]:\n",
    "    dataset_test = ir_datasets.load(DATASETS[name]['ir_datasets_test_id'])\n",
    "    \n",
    "    queries = list(dataset_test.queries_iter())\n",
    "    qrels = list(dataset_test.qrels_iter())\n",
    "    return queries, qrels\n",
    "\n",
    "def load_dataset_with_queries(name: str) -> tuple[list[Doc], list[Query], list[Qrel]]:\n",
    "    dataset = ir_datasets.load(DATASETS[name]['ir_datasets_id'])\n",
    "    dataset_test = ir_datasets.load(DATASETS[name]['ir_datasets_test_id'])\n",
    "    \n",
    "    docs = list(dataset.docs_iter())\n",
    "    queries = list(dataset_test.queries_iter())\n",
    "    qrels = list(dataset_test.qrels_iter())\n",
    "    \n",
    "    return docs, queries, qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312de48b",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "\n",
    "class TextPreprocessor:\n",
    "    __lemmatizer__ = WordNetLemmatizer()\n",
    "    __stop_words__ = set(stopwords.words('english'))\n",
    "    __instance__ = None\n",
    "\n",
    "    @staticmethod\n",
    "    def getInstance():\n",
    "        if TextPreprocessor.__instance__ == None:\n",
    "            TextPreprocessor.__instance__ = TextPreprocessor()\n",
    "        return TextPreprocessor.__instance__\n",
    "\n",
    "    def __clean_text__(self, text):\n",
    "        \"\"\"\n",
    "        Clean text by removing special characters and converting to lowercase\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def __remove_stopwords__(self, text):\n",
    "        \"\"\"\n",
    "        Remove common stopwords from text\n",
    "        \"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word not in self.__stop_words__]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def __get_wordnet_pos__(self, tag_parameter):\n",
    "        tag = tag_parameter[0].upper()\n",
    "        tag_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV\n",
    "            }\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def __lemmatize_text__(self, text):\n",
    "        \"\"\"\n",
    "        Lemmatize words to their root form\n",
    "        \"\"\"\n",
    "        # Tokenize into words\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # POS tagging\n",
    "        pos_tags = pos_tag(words)\n",
    "\n",
    "        lemmatized_words = [self.__lemmatizer__.lemmatize(word, pos=self.__get_wordnet_pos__(tag)) for word, tag in pos_tags]\n",
    "\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Apply full preprocessing pipeline\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Apply all preprocessing steps\n",
    "        text = self.__clean_text__(text)\n",
    "        text = self.__lemmatize_text__(text)\n",
    "        text = self.__remove_stopwords__(text)\n",
    "\n",
    "        return word_tokenize(text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21122f2",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(set)        # term -> {doc_id}\n",
    "        self.doc_lengths = defaultdict(int)  # doc_id -> total terms\n",
    "        self.N = 0                           # total documents\n",
    "\n",
    "    def add_document(self, doc_id, tokens):\n",
    "        self.N += 1\n",
    "        self.doc_lengths[doc_id] = len(tokens)\n",
    "        for token in tokens:\n",
    "            self.index[token].add(doc_id)\n",
    "\n",
    "    def get_documents_sharing_terms_with_query(self, query_tokens):\n",
    "        \"\"\"\n",
    "        Returns a set of doc_ids that share at least one word with the query.\n",
    "        \"\"\"\n",
    "        related_docs = set()\n",
    "\n",
    "        for token in query_tokens:\n",
    "            related_docs.update(self.index.get(token, set()))\n",
    "\n",
    "        return list(related_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55eba8",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68915fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_dcg(relevance, rank):\n",
    "    return ((2 ** relevance) - 1) / math.log10(rank + 1)\n",
    "\n",
    "class Retriever:\n",
    "    def search(self, dataset_name: str, query: str, top_k: int = 10, with_index: bool = True) -> list[tuple[str, float, str]]:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def evaluateNDCG(self, dataset_name, queries, qrels, docs, K = 10, print_more = False):\n",
    "        nDCG = []\n",
    "\n",
    "        for i in range(len(queries)):\n",
    "            query = queries[i]\n",
    "            if print_more:\n",
    "                preprocess_text = TextPreprocessor.getInstance().preprocess_text\n",
    "                print(f\"Query: {query.text}\")\n",
    "                print(f\"Query: {preprocess_text(query.text)}\")\n",
    "            \n",
    "            # Search\n",
    "            results = self.search(dataset_name, query.text, K, True)\n",
    "            if print_more:\n",
    "                for i, res in enumerate(results):\n",
    "                    print(f\"Result #{i} {res[1]}: {res[2]}\")\n",
    "                    print(f\"Result #{i} {res[1]}: {preprocess_text(res[2])}\")\n",
    "\n",
    "            # Find relevant documents for this query\n",
    "            relevant_qrels = [qrel for qrel in qrels if qrel.query_id == query.query_id]\n",
    "            relevant_qrels = sorted(relevant_qrels, key=lambda x: x.relevance, reverse=True)\n",
    "            if print_more:\n",
    "                for i, qrel in enumerate(relevant_qrels[:K]):\n",
    "                    doc = [doc for doc in docs if qrel.doc_id == doc.doc_id][0]\n",
    "                    print(f\"Qrel #{i} {qrel.relevance}: {doc.text}\")\n",
    "                    print(f\"Qrel #{i} {qrel.relevance}: {preprocess_text(doc.text)}\")\n",
    "            \n",
    "            DCG = [\n",
    "                calc_dcg(\n",
    "                    list(\n",
    "                        filter(\n",
    "                            lambda qrel: qrel.doc_id == doc[0], relevant_qrels\n",
    "                            )\n",
    "                        )[0].relevance if list(\n",
    "                        filter(\n",
    "                            lambda qrel: qrel.doc_id == doc[0], relevant_qrels\n",
    "                            )\n",
    "                        ) else 0\n",
    "                    , i+1\n",
    "                ) for i, doc in enumerate(results)]\n",
    "            \n",
    "            iDCG = [calc_dcg(qrel.relevance, i+1) for i, qrel in enumerate(relevant_qrels[:K])]\n",
    "            \n",
    "            res = sum(DCG) \n",
    "            ires = sum(iDCG) \n",
    "            \n",
    "            if print_more:\n",
    "                print(\"\")\n",
    "                print(f\"query: {i+1}/{len(queries)}\")\n",
    "                print(f\"DCG: {res}\")\n",
    "                print(f\"iDCG: {ires}\")\n",
    "                print(f\"nDCG: {res/ires*100}%\")\n",
    "            nDCG.append(res/ires)\n",
    "            if print_more:\n",
    "                print(f\"Average nDCG: {sum(nDCG)/len(nDCG)*100}%\")\n",
    "        \n",
    "        nDCG = sum(nDCG)/len(nDCG)*100\n",
    "\n",
    "        if print_more:\n",
    "            print(f\"Final Average nDCG: {nDCG}%\")\n",
    "\n",
    "        return nDCG\n",
    "\n",
    "    def evaluateMRR(self, dataset_name, queries, qrels, K = 100, print_more = False):\n",
    "        MRR = []\n",
    "\n",
    "        cleaned_qrels: dict[str, dict[str, int]] = {}\n",
    "        for qrel in qrels:\n",
    "            if qrel.query_id not in cleaned_qrels.keys():\n",
    "                cleaned_qrels[qrel.query_id] = {}\n",
    "            cleaned_qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
    "\n",
    "        for i in range(len(queries)):\n",
    "            query = queries[i]\n",
    "            results = self.search(dataset_name, query.text, K, True)\n",
    "            \n",
    "            firstRank = 100\n",
    "            for ii, res in enumerate(results):\n",
    "                if res[0] in cleaned_qrels[query.query_id].keys() and cleaned_qrels[query.query_id][res[0]] > 0:\n",
    "                    firstRank = ii + 1\n",
    "            \n",
    "            MRR.append(1/firstRank)\n",
    "            \n",
    "            if print_more:\n",
    "                print()\n",
    "                print(f\"Query: {i+1}/{len(queries)}\")\n",
    "                print(f\"Current MRR: {sum(MRR) / len(MRR) * 100}\")\n",
    "        \n",
    "        MRR = sum(MRR) / len(MRR) * 100\n",
    "        if print_more:\n",
    "            print(f\"MRR: {MRR}%\")\n",
    "        return MRR\n",
    "    \n",
    "    def evaluateMAP(self, dataset_name, queries, qrels,docs, K = 10, print_more = False):\n",
    "        MAP = []\n",
    "\n",
    "        cleaned_qrels: dict[str, dict[str, int]] = {}\n",
    "        for qrel in qrels:\n",
    "            if qrel.query_id not in cleaned_qrels.keys():\n",
    "                cleaned_qrels[qrel.query_id] = {}\n",
    "            cleaned_qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
    "        \n",
    "        for i in range(len(queries)):\n",
    "            query = queries[i]\n",
    "            if print_more:\n",
    "                print()\n",
    "                print(f'Query: {i+1}/{len(queries)}')\n",
    "                print(query.text)\n",
    "\n",
    "            results = self.search(dataset_name, query.text, K, True)\n",
    "            if print_more:\n",
    "                print([res[0] for res in results])\n",
    "                print([qrel.doc_id+f\": {qrel.relevance}\" for qrel in qrels if qrel.query_id == query.query_id])\n",
    "                print(\"results\")\n",
    "                for doc in [doc for doc in docs if doc.doc_id in [res[0] for res in results]]:\n",
    "                    print(doc.doc_id+\" \"+doc.text)\n",
    "                print(\"qrels\")\n",
    "                koko = [qrel.doc_id for qrel in qrels if qrel.query_id == query.query_id]\n",
    "                for doc in [doc for doc in docs if doc.doc_id in koko]:\n",
    "                    print(doc.doc_id+\" \"+doc.text)\n",
    "\n",
    "            relevant_num = 0\n",
    "            precision_sum = 0\n",
    "            for ii, res in enumerate(results):\n",
    "                if res[0] in cleaned_qrels[query.query_id].keys() and cleaned_qrels[query.query_id][res[0]] > 0:\n",
    "                    relevant_num += 1\n",
    "                    precision_sum += relevant_num / (ii + 1)\n",
    "            if print_more:\n",
    "                print(precision_sum)\n",
    "            if relevant_num > 0:\n",
    "                MAP.append(precision_sum / relevant_num)\n",
    "            if print_more:\n",
    "                if len(MAP) > 0:\n",
    "                    print(f'MAP = {sum(MAP) / len(MAP) * 100}')\n",
    "        if len(MAP) > 0:\n",
    "            MAP = sum(MAP) / len(MAP) * 100\n",
    "        else:\n",
    "            MAP = 0\n",
    "        if print_more:\n",
    "            print(f'MAP={MAP}%')\n",
    "        return MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import dill\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TFIDF_online(Retriever):\n",
    "    __tfidfInstance__ : dict[str, list] = {}\n",
    "    __invertedIndex__ : dict[str, InvertedIndex] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def __loadInstance__(dataset_name : str):\n",
    "        if dataset_name not in TFIDF_online.__tfidfInstance__.keys():\n",
    "\n",
    "            # Load the model and the documents\n",
    "            docs = load_dataset(dataset_name)\n",
    "            vectorizer = joblib.load(f\"../data/{dataset_name}/tfidf_vectorizer.joblib\")\n",
    "            docs_tfidf_matrix = joblib.load(f\"../data/{dataset_name}/tfidf_matrix.joblib\")\n",
    "\n",
    "            TFIDF_online.__tfidfInstance__[dataset_name] = [docs,vectorizer,docs_tfidf_matrix]\n",
    "\n",
    "    @staticmethod\n",
    "    def __loadInvertedIndex__(dataset_name : str):\n",
    "        if dataset_name not in TFIDF_online.__invertedIndex__.keys():\n",
    "            with open(f\"../data/{dataset_name}/inverted_index.dill\", \"rb\") as f:\n",
    "                inverted_index = InvertedIndex()\n",
    "                ii = dill.load(f)\n",
    "                inverted_index.index = ii.index\n",
    "                inverted_index.doc_lengths = ii.doc_lengths\n",
    "                inverted_index.N = ii.N\n",
    "                TFIDF_online.__invertedIndex__[dataset_name] = inverted_index\n",
    "\n",
    "    def search(self, dataset_name, query, top_k, with_index = True):\n",
    "\n",
    "        # Load the model and the index\n",
    "        self.__loadInstance__(dataset_name)\n",
    "        docs = self.__tfidfInstance__[dataset_name][0]\n",
    "        vectorizer = self.__tfidfInstance__[dataset_name][1]\n",
    "        docs_tfidf_matrix = self.__tfidfInstance__[dataset_name][2]\n",
    "        \n",
    "        self.__loadInvertedIndex__(dataset_name)\n",
    "        inverted_index = self.__invertedIndex__[dataset_name]\n",
    "\n",
    "        # Start the process\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        \n",
    "        if(with_index):\n",
    "            tokenized_query = TextPreprocessor.getInstance().preprocess_text(query)\n",
    "            candidate_indices = inverted_index.get_documents_sharing_terms_with_query(tokenized_query)   \n",
    "            docs_tfidf_matrix = docs_tfidf_matrix[candidate_indices]\n",
    "\n",
    "        cosine_sim = cosine_similarity(query_vec, docs_tfidf_matrix).flatten()\n",
    "\n",
    "        ranked_indices = np.argsort(cosine_sim)[::-1]\n",
    "\n",
    "        # Prepare structured results\n",
    "        results = []\n",
    "        # Limit results to a reasonable number for display/API response, e.g., top 10 or 20\n",
    "        for i in ranked_indices[:top_k]:\n",
    "            if(with_index):\n",
    "                original_doc_idx = candidate_indices[i]\n",
    "            else:\n",
    "                original_doc_idx = i\n",
    "\n",
    "            doc = docs[original_doc_idx]\n",
    "            results.append((\n",
    "                docs[original_doc_idx].doc_id,\n",
    "                float(cosine_sim[i]),\n",
    "                doc.text[:40] + \"...\" if len(doc.text) > 40 else doc.text # Provide a snippet\n",
    "            ))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Embedding_online(Retriever):\n",
    "    __embeddingInstance__ : dict[str, any] = {}\n",
    "    __collection_instance__: dict = {}\n",
    "    __modelInstance__  = None\n",
    "\n",
    "    @staticmethod\n",
    "    def __loadModelInstance__():\n",
    "        if Embedding_online.__modelInstance__ == None:\n",
    "            Embedding_online.__modelInstance__ = SentenceTransformer(\"../data/models/all-MiniLM-L6-v2\") \n",
    "        return Embedding_online.__modelInstance__\n",
    "\n",
    "    @staticmethod\n",
    "    def __loadInstance__(dataset_name : str):\n",
    "        if dataset_name not in Embedding_online.__embeddingInstance__.keys():\n",
    "            with open(f\"../data/{dataset_name}/bert_embeddings.npy\", \"rb\") as f:\n",
    "                Embedding_online.__embeddingInstance__[dataset_name] = np.load(f)\n",
    "        return Embedding_online.__embeddingInstance__[dataset_name]\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_collection__(dataset_name: str):\n",
    "        if dataset_name not in Embedding_online.__collection_instance__:\n",
    "            print(f\"Connecting to ChromaDB and getting collection: {dataset_name}_embeddings...\")\n",
    "            client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "            Embedding_online.__collection_instance__[dataset_name] = client.get_collection(name=f\"{dataset_name}_embeddings\")\n",
    "        return Embedding_online.__collection_instance__[dataset_name]\n",
    "\n",
    "    def search(self, dataset_name: str, query: str, top_k: int = 10, with_index: bool = True):\n",
    "        if with_index:\n",
    "            return self.embedding_vectors_search(dataset_name, query, top_k)\n",
    "        else:\n",
    "            return self.embedding_search(dataset_name, query, top_k)\n",
    "\n",
    "    def embedding_search(self, dataset_name: str, query: str, top_k: int):\n",
    "        # Load model and documents\n",
    "        model = Embedding_online.__loadModelInstance__()\n",
    "        document_embeddings =  Embedding_online.__loadInstance__(dataset_name)\n",
    "        docs = load_dataset(dataset_name)\n",
    "        processedQuery = TextPreprocessor.getInstance().preprocess_text(query)\n",
    "        query_embedding = model.encode(processedQuery)\n",
    "\n",
    "        cos_scores = util.cos_sim(torch.tensor(query_embedding), torch.tensor(document_embeddings))[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "        results = []\n",
    "        # print(f\"\\nTop {top_k} results for query: '{query}'\")\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            doc_id = docs[idx].doc_id\n",
    "            doc_text = docs[idx].text[:100] + \"...\" \n",
    "            results.append((doc_id, score.item(), doc_text))\n",
    "            # print(f\"Doc ID: {doc_id}, Score: {score.item():.4f}, Text: {doc_text}\"\n",
    "        return results\n",
    "\n",
    "    def embedding_vectors_search(self, dataset_name: str, query: str, top_k: int):\n",
    "        #Load model and collection\n",
    "        model = Embedding_online.__loadModelInstance__()\n",
    "        collection = Embedding_online.__get_collection__(dataset_name)\n",
    "        #process query\n",
    "        processedQuery = TextPreprocessor.getInstance().preprocess_text(query)\n",
    "        query_embedding = model.encode(processedQuery)\n",
    "        search_results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        results = []\n",
    "        ids = search_results['ids'][0]\n",
    "        distances = search_results['distances'][0]\n",
    "        metadatas = search_results['metadatas'][0]\n",
    "        for doc_id, score, meta in zip(ids, distances, metadatas):\n",
    "            similarity_score = 1 - score\n",
    "            text = meta.get('text', '')[:100] + \"...\" \n",
    "            results.append((doc_id, similarity_score, text))\n",
    "        return results\n",
    "\n",
    "    def embedding_rerank(self, dataset_name: str, query: str, doc_ids: list) -> list[tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Efficiently re-ranks a list of documents using the loaded embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        docs_list = load_dataset(dataset_name)\n",
    "        document_embeddings =  Embedding_online.__loadInstance__(dataset_name)\n",
    "        model = Embedding_online.__loadModelInstance__()\n",
    "\n",
    "        # 1. Create a quick lookup map for doc_id to its index\n",
    "        doc_id_to_index = enumerate(docs_list)\n",
    "\n",
    "        # 2. Get the indices and embeddings for the documents we need to rerank\n",
    "        candidate_indices = [i for i, doc in doc_id_to_index if doc.doc_id in doc_ids]\n",
    "\n",
    "        # Filter out any docs that might not be in our list\n",
    "        valid_indices = [idx for idx in candidate_indices if idx is not None]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            return []\n",
    "            \n",
    "        candidate_embeddings = document_embeddings[valid_indices]\n",
    "        \n",
    "        # 3. Encode the query\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # 4. Calculate similarity scores\n",
    "        cosine_scores = util.cos_sim(query_embedding, candidate_embeddings)[0]\n",
    "        \n",
    "        # 5. Pair the original doc_ids (that were valid) with their new scores\n",
    "        valid_doc_ids = [docs_list[i] for i in valid_indices]\n",
    "        reranked_results = []\n",
    "        for doc_id, score in zip(valid_doc_ids, cosine_scores):\n",
    "            reranked_results.append((doc_id, score.item()))\n",
    "\n",
    "        return sorted(reranked_results, key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import dill\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "class BM25_online(Retriever):\n",
    "    __bm25instance__ : dict[str, BM25Okapi] = {}\n",
    "    __invertedIndex__ : dict[str, InvertedIndex] = {}\n",
    "    __docs__ : dict[str, list[str, str]] = {}\n",
    "    @staticmethod\n",
    "    def __loadInstance__(dataset_name : str):\n",
    "        if dataset_name not in BM25_online.__bm25instance__.keys():\n",
    "            with open(os.path.join(os.path.abspath(''), f\"../data/{dataset_name}/bm25_model.dill\"), \"rb\") as f:\n",
    "                BM25_online.__bm25instance__[dataset_name] = dill.load(f) \n",
    "        return BM25_online.__bm25instance__[dataset_name]\n",
    "    @staticmethod\n",
    "    def __loadInvertedIndex__(dataset_name : str):\n",
    "        if dataset_name not in BM25_online.__invertedIndex__.keys():\n",
    "            with open(f\"../data/{dataset_name}/inverted_index.dill\", \"rb\") as f:\n",
    "                inverted_index = InvertedIndex()\n",
    "                ii = dill.load(f)\n",
    "                inverted_index.index = ii.index\n",
    "                inverted_index.doc_lengths = ii.doc_lengths\n",
    "                inverted_index.N = ii.N\n",
    "                BM25_online.__invertedIndex__[dataset_name] = inverted_index\n",
    "        return BM25_online.__invertedIndex__[dataset_name]\n",
    "    @staticmethod\n",
    "    def __loadDocs__(dataset_name : str):\n",
    "        if dataset_name not in BM25_online.__docs__.keys():\n",
    "            BM25_online.__docs__[dataset_name] = load_dataset(dataset_name)\n",
    "        return BM25_online.__docs__[dataset_name]\n",
    "\n",
    "    def search(self, dataset_name: str, query: str, top_k: int = 10, with_inverted_index: bool = True) -> list[tuple[str, float, str]]:\n",
    "        # Load the model and the documents\n",
    "        bm25 = BM25_online.__loadInstance__(dataset_name)\n",
    "        docs = BM25_online.__loadDocs__(dataset_name)\n",
    "        if with_inverted_index:\n",
    "            inverted_index = BM25_online.__loadInvertedIndex__(dataset_name)\n",
    "\n",
    "        # Execute the query\n",
    "        query_tokens = TextPreprocessor.getInstance().preprocess_text(query)\n",
    "\n",
    "        if with_inverted_index:\n",
    "            documents_sharing_terms_with_query = inverted_index.get_documents_sharing_terms_with_query(query_tokens)\n",
    "            scores = bm25.get_batch_scores(query_tokens, documents_sharing_terms_with_query)\n",
    "        else:\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "        # Sort the results\n",
    "        if with_inverted_index:\n",
    "            top_indices = sorted(list(enumerate(documents_sharing_terms_with_query)), key=lambda  elm: scores[elm[0]], reverse=True)[:top_k]\n",
    "        else:\n",
    "            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Display the results\n",
    "        if with_inverted_index:\n",
    "            for elm in top_indices:\n",
    "                text = docs[elm[1]].text\n",
    "                results.append((docs[elm[1]].doc_id, scores[elm[0]], text))\n",
    "        else:\n",
    "            for idx in top_indices:\n",
    "                text = docs[idx].text\n",
    "                results.append((docs[idx].doc_id, scores[idx], text))\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_online(Retriever):\n",
    "    def __normalize_scores__(self, ranked_list: list) -> list:\n",
    "        \"\"\"\n",
    "        Normalizes scores in a ranked list to a [0, 1] scale.\n",
    "        \"\"\"\n",
    "        scores = [score for doc_id, score, text in ranked_list]\n",
    "        if not scores:\n",
    "            return []\n",
    "        \n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        \n",
    "        if max_score == min_score:\n",
    "            return [(doc_id, 1.0) for doc_id, score, text in ranked_list]\n",
    "        \n",
    "        normalized_list = []\n",
    "        for doc_id, score, text in ranked_list:\n",
    "            normalized_score = (score - min_score) / (max_score - min_score)\n",
    "            normalized_list.append((doc_id, normalized_score))\n",
    "        return normalized_list\n",
    "\n",
    "    def search(self, dataset_name: str, query: str, top_k: int = 10, with_index: bool = True) -> list[tuple[str, float, str]]:\n",
    "        \"\"\"\n",
    "        Performs a complex hybrid search.\n",
    "        \"\"\"\n",
    "        print(\"\\nPerforming complex hybrid search (Stage 1: Fusion)...\")\n",
    "\n",
    "        # --- Get the required service modules from the registry ---\n",
    "        tfidf_service = TFIDF_online()\n",
    "        bm25_service = BM25_online()\n",
    "        embedding_service = Embedding_online()\n",
    "\n",
    "        # ==========================================================================\n",
    "        #  STAGE 1: Parallel Fusion of TF-IDF and BM25\n",
    "        # ==========================================================================\n",
    "        \n",
    "        tfidf_results = tfidf_service.search(dataset_name, query, top_k)\n",
    "\n",
    "        bm25_results = bm25_service.search(dataset_name, query, top_k)\n",
    "\n",
    "        # --- Normalize and Fuse the lexical results ---\n",
    "        norm_tfidf = self.__normalize_scores__(tfidf_results)\n",
    "        norm_bm25 = self.__normalize_scores__(bm25_results)\n",
    "\n",
    "        fused_scores = {}\n",
    "        tfidf_weight = 0.5\n",
    "        bm25_weight = 0.5\n",
    "\n",
    "        for doc_id, score in norm_tfidf:\n",
    "            fused_scores[str(doc_id)] = score * tfidf_weight\n",
    "\n",
    "        for doc_id, score in norm_bm25:\n",
    "            doc_id_str = str(doc_id)\n",
    "            if doc_id_str in fused_scores:\n",
    "                fused_scores[doc_id_str] += score * bm25_weight\n",
    "            else:\n",
    "                fused_scores[doc_id_str] = score * bm25_weight\n",
    "                \n",
    "        # --- Create the final candidate list from Stage 1 ---\n",
    "        candidate_list = sorted(fused_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        # Extract just the document IDs for the next stage\n",
    "        candidate_doc_ids = [doc_id for doc_id, score in candidate_list]\n",
    "        \n",
    "        # ==========================================================================\n",
    "        #  STAGE 2: Serial Re-ranking with Embedding Model\n",
    "        # ==========================================================================\n",
    "\n",
    "        # Call the new, efficient rerank function from the embedding service\n",
    "        final_list = embedding_service.embedding_rerank(dataset_name, query, candidate_doc_ids)\n",
    "        \n",
    "        return final_list[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad472f7",
   "metadata": {},
   "source": [
    "## run main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02006705",
   "metadata": {},
   "source": [
    "### Antique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62591ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'antique'\n",
    "\n",
    "docs, queries, qrels = load_dataset_with_queries(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bd554",
   "metadata": {},
   "source": [
    "### Antique tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d076b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Calculating MAP for tf-idf\")\n",
    "st = time.time()\n",
    "antique_tfidf_MAP = TFIDF_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"tf-idf MAP= {antique_tfidf_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for tf-idf\")\n",
    "st = time.time()\n",
    "antique_tfidf_MRR = TFIDF_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"tf-idf MRR= {antique_tfidf_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for tf-idf\")\n",
    "st = time.time()\n",
    "antique_tfidf_NDCG = TFIDF_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"tf-idf nDCG= {antique_tfidf_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba6218",
   "metadata": {},
   "source": [
    "### Antique Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Calculating MAP for embedding\")\n",
    "st = time.time()\n",
    "antique_embedding_MAP = Embedding_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"embedding MAP= {antique_embedding_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for embedding\")\n",
    "st = time.time()\n",
    "antique_embedding_MRR = Embedding_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"embedding MRR= {antique_embedding_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for embedding\")\n",
    "st = time.time()\n",
    "antique_embedding_NDCG = Embedding_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"embedding nDCG= {antique_embedding_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0024e",
   "metadata": {},
   "source": [
    "### Antique bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76356a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Calculating MAP for bm25\")\n",
    "st = time.time()\n",
    "antique_bm25_MAP = BM25_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"bm25 MAP= {antique_bm25_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for bm25\")\n",
    "st = time.time()\n",
    "antique_bm25_MRR = BM25_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"bm25 MRR= {antique_bm25_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for bm25\")\n",
    "st = time.time()\n",
    "antique_bm25_NDCG = BM25_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"bm25 nDCG= {antique_bm25_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c84c8",
   "metadata": {},
   "source": [
    "### Antique hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def312db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Calculating MAP for hybrid\")\n",
    "st = time.time()\n",
    "antique_hybrid_MAP = Hybrid_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"hybrid MAP= {antique_hybrid_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for hybrid\")\n",
    "st = time.time()\n",
    "antique_hybrid_MRR = Hybrid_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"hybrid MRR= {antique_hybrid_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for hybrid\")\n",
    "st = time.time()\n",
    "antique_hybrid_NDCG = Hybrid_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"hybrid nDCG= {antique_hybrid_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86baa4f0",
   "metadata": {},
   "source": [
    "### Quora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ec27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'quora'\n",
    "\n",
    "docs, queries, qrels = load_dataset_with_queries(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8f241",
   "metadata": {},
   "source": [
    "### Quora tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a241474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Calculating MAP for tf-idf\")\n",
    "st = time.time()\n",
    "quora_tfidf_MAP = TFIDF_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"tf-idf MAP= {quora_tfidf_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for tf-idf\")\n",
    "st = time.time()\n",
    "quora_tfidf_MRR = TFIDF_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"tf-idf MRR= {quora_tfidf_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for tf-idf\")\n",
    "st = time.time()\n",
    "quora_tfidf_NDCG = TFIDF_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"tf-idf nDCG= {quora_tfidf_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c708f",
   "metadata": {},
   "source": [
    "### Quora Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Calculating MAP for Embedding\")\n",
    "st = time.time()\n",
    "quora_embedding_MAP = Embedding_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"Embedding MAP= {quora_embedding_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for Embedding\")\n",
    "st = time.time()\n",
    "quora_embedding_MRR = Embedding_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"Embedding MRR= {quora_embedding_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for Embedding\")\n",
    "st = time.time()\n",
    "quora_embedding_NDCG = Embedding_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"Embedding nDCG= {quora_embedding_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ed57",
   "metadata": {},
   "source": [
    "### Quora bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea55d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Calculating MAP for bm25\")\n",
    "st = time.time()\n",
    "quora_bm25_MAP = BM25_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"bm25 MAP= {quora_bm25_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for bm25\")\n",
    "st = time.time()\n",
    "quora_bm25_MRR = BM25_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"bm25 MRR= {quora_bm25_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for bm25\")\n",
    "st = time.time()\n",
    "quora_bm25_NDCG = BM25_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"bm25 nDCG= {quora_bm25_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674ca2b",
   "metadata": {},
   "source": [
    "### Quora Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "print(\"Calculating MAP for Hybrid\")\n",
    "st = time.time()\n",
    "quora_hybrid_MAP = Hybrid_online().evaluateMAP(dataset_name, queries, qrels, docs)\n",
    "print(f\"Hybrid MAP= {quora_hybrid_MAP}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating MRR for Hybrid\")\n",
    "st = time.time()\n",
    "quora_hybrid_MRR = Hybrid_online().evaluateMRR(dataset_name, queries, qrels)\n",
    "print(f\"Hybrid MRR= {quora_hybrid_MRR}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Calculating nDCG for Hybrid\")\n",
    "st = time.time()\n",
    "quora_hybrid_NDCG = Hybrid_online().evaluateNDCG(dataset_name, queries, qrels, docs)\n",
    "print(f\"Hybrid nDCG= {quora_hybrid_NDCG}\")\n",
    "print(f\"This took {time.time() - st}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357d1b8",
   "metadata": {},
   "source": [
    "### Collecting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "antique_values = [\n",
    "[antique_tfidf_MRR, antique_embedding_MRR, antique_bm25_MRR, antique_hybrid_MRR],\n",
    "[antique_tfidf_MAP, antique_embedding_MAP, antique_bm25_MAP, antique_hybrid_MAP],\n",
    "[antique_tfidf_NDCG, antique_embedding_NDCG, antique_bm25_NDCG, antique_hybrid_NDCG],\n",
    "]\n",
    "\n",
    "quora_values = [\n",
    "[quora_tfidf_MRR, quora_embedding_MRR, quora_bm25_MRR, quora_hybrid_MRR],\n",
    "[quora_tfidf_MAP, quora_embedding_MAP, quora_bm25_MAP, quora_hybrid_MAP],\n",
    "[quora_tfidf_NDCG, quora_embedding_NDCG, quora_bm25_NDCG, quora_hybrid_NDCG],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff80253",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "methods = [\"tf-idf\", \"embedding\", \"hybrid\", \"bm25\"]\n",
    "metrics = [\"MRR\", \"MAP\", \"nDCG\"]\n",
    "x_pos = np.arange(len(methods))\n",
    "y_pos = np.arange(len(metrics))\n",
    "\n",
    "def draw_plot(dataset_name, values):\n",
    "\n",
    "    plt.figure(figsize = (10, 6))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.scatter(x_pos, [y_pos[i]]*len(methods), s = values[i]*3000, alpha = 0.6, label = metrics[i])\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "        for j in range(len(methods)):\n",
    "            plt.annotate(f\"{values[i, j]:.2f}\",\n",
    "                        (x_pos[j], y_pos[i]),\n",
    "                        ha = 'center', va = 'center',\n",
    "                        fontsize = 10\n",
    "                        )\n",
    "\n",
    "    plt.xticks(x_pos, methods)\n",
    "    plt.yticks(y_pos, metrics)\n",
    "\n",
    "    plt.title(dataset_name, pad = 20)\n",
    "    plt.xlabel(\"Retrieval Methods\")\n",
    "    plt.ylabel(\"Evaluation Metrics\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, linestyle = '--', alpha = 0.7)\n",
    "    plt.legend(bbox_to_anchor = (1.05, 1), loc = 'upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(\"antique\", antique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf36f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(\"quora\", quora_values)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
