{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6bc018",
   "metadata": {
    "id": "3e6bc018"
   },
   "source": [
    "## Libraries Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b28b37",
   "metadata": {
    "id": "b6b28b37"
   },
   "source": [
    "### NLTK Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f5cba",
   "metadata": {
    "id": "748f5cba",
    "outputId": "af4a1421-49e7-478f-8fd7-106a07a61f48"
   },
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe9f56",
   "metadata": {
    "id": "c4fe9f56",
    "outputId": "ec1f4d82-e22b-4c04-8ed4-8e7d4382de39"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f7283",
   "metadata": {},
   "source": [
    "### ir_datasets Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce88515",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ir_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08b835",
   "metadata": {},
   "source": [
    "### rank_bm25 Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395e9c0",
   "metadata": {},
   "source": [
    "### joblib Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6352f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ff51a",
   "metadata": {},
   "source": [
    "### dill Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce9628",
   "metadata": {},
   "source": [
    "### contractions Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d98520",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead68964",
   "metadata": {},
   "source": [
    "### chromadb Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19ece0",
   "metadata": {},
   "source": [
    "### scikit-learn Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef88c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc62c0",
   "metadata": {},
   "source": [
    "### matplotlib Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32707abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38455b5",
   "metadata": {},
   "source": [
    "### fastapi Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32971561",
   "metadata": {},
   "source": [
    "## Apply ir_datasets fix for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95df819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace the user's path with the correct path\n",
    "ir_datasets_path = r\"C:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ir_datasets\\formats\\tsv.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "if not os.path.exists(ir_datasets_path):\n",
    "    print(f\"Error: File not found at {ir_datasets_path}\")\n",
    "    exit()\n",
    "\n",
    "# Read the file content\n",
    "with open(ir_datasets_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Replace lines 25-28 with new code\n",
    "    new_lines = [\n",
    "        \"            if isinstance(self.dlc, list):\\n\",\n",
    "        \"                self.stream = io.TextIOWrapper(self.ctxt.enter_context(self.dlc[self.stream_idx].stream()), encoding='utf-8')\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                self.stream = io.TextIOWrapper(self.ctxt.enter_context(self.dlc.stream()), encoding='utf-8')\\n\"\n",
    "    ]\n",
    "    \n",
    "    # Replace the lines (note: list indices are 0-based, so lines 25-28 are indices 24-27)\n",
    "    lines[24:28] = new_lines\n",
    "    \n",
    "    # Write back the modified content\n",
    "    with open(ir_datasets_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(lines)\n",
    "    print(\"Successfully updated the file encoding settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32707abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load TF-IDF matrix from: C:\\Users\\FSOS\\Documents\\Projects\\ir-project\\src\\data\\antique\\tfidf_matrix.joblib\n",
      "✅ Successfully loaded TF-IDF matrix.\n",
      "TF-IDF Matrix shape (num_docs, vocab_size): (403666, 35560)\n",
      "\n",
      "Calculating cumulative explained variance for n_components from 1 to 500...\n",
      "  Calculated for n_components = 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     43\u001b[39m     svd = TruncatedSVD(n_components=n, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43msvd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     cumulative_explained_variance = np.sum(svd.explained_variance_ratio_)\n\u001b[32m     46\u001b[39m     explained_variance_ratios.append(cumulative_explained_variance)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:206\u001b[39m, in \u001b[36mTruncatedSVD.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit model on training data X.\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    204\u001b[39m \u001b[33;03m        Returns the transformer object.\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:262\u001b[39m, in \u001b[36mTruncatedSVD.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# X @ V is not the same as U @ Sigma\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.algorithm == \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.algorithm == \u001b[33m\"\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tol > \u001b[32m0\u001b[39m\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     X_transformed = \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomponents_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     X_transformed = U * Sigma\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:203\u001b[39m, in \u001b[36msafe_sparse_dot\u001b[39m\u001b[34m(a, b, dense_output)\u001b[39m\n\u001b[32m    201\u001b[39m         ret = xp.tensordot(a, b, axes=[-\u001b[32m1\u001b[39m, b_axis])\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     ret = \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    206\u001b[39m     sparse.issparse(a)\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m sparse.issparse(b)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[32m    209\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[33m\"\u001b[39m\u001b[33mtoarray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m ):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.toarray()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:905\u001b[39m, in \u001b[36m_spbase.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScalar operands are not allowed, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33muse \u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matmul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:797\u001b[39m, in \u001b[36m_spbase._matmul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    795\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(M, \u001b[32m1\u001b[39m)\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m other.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other.shape[\u001b[32m0\u001b[39m] == N:\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matmul_multivector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    800\u001b[39m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mul_scalar(other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FSOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:408\u001b[39m, in \u001b[36m_cs_matrix._matmul_multivector\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[32m    407\u001b[39m fn = \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m.format + \u001b[33m'\u001b[39m\u001b[33m_matvecs\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m   \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape((n_vecs,))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.utils import resample\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors # To help estimate eps for DBSCAN\n",
    "\n",
    "# --- Path to your dimensionality-reduced data (Google Drive) ---\n",
    "google_drive_model_path = \"C:\\\\Users\\\\FSOS\\\\Documents\\\\Projects\\\\ir-project\\\\src\\\\data\\\\antique\"\n",
    "data_dir_path = Path(google_drive_model_path)\n",
    "\n",
    "tfidf_matrix_reduced_svd_path = data_dir_path / \"tfidf_matrix_reduced_svd.joblib\"\n",
    "tfidf_matrix_reduced_umap_path = data_dir_path / \"tfidf_matrix_reduced_umap.joblib\" # For later use\n",
    "\n",
    "tfidf_matrix_reduced_svd = None\n",
    "tfidf_matrix_reduced_umap = None\n",
    "\n",
    "print(f\"--- Attempting to load dimensionality-reduced matrices from: {data_dir_path.resolve()} ---\")\n",
    "\n",
    "# Load SVD-reduced matrix\n",
    "if not tfidf_matrix_reduced_svd_path.exists():\n",
    "    print(f\"❌ SVD-reduced matrix NOT FOUND at {tfidf_matrix_reduced_svd_path.resolve()}. Please ensure TruncatedSVD was run successfully.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"\\nAttempting joblib.load for SVD-reduced matrix...\")\n",
    "        tfidf_matrix_reduced_svd = joblib.load(tfidf_matrix_reduced_svd_path)\n",
    "        print(\"✅ SVD-reduced matrix loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Critical Error loading SVD-reduced matrix: {e}.\")\n",
    "\n",
    "# Load UMAP-reduced matrix (optional, for later use)\n",
    "if tfidf_matrix_reduced_umap_path.exists():\n",
    "     try:\n",
    "        print(\"\\nAttempting joblib.load for UMAP-reduced matrix...\")\n",
    "        tfidf_matrix_reduced_umap = joblib.load(tfidf_matrix_reduced_umap_path)\n",
    "        print(\"✅ UMAP-reduced matrix loaded successfully.\")\n",
    "     except Exception as e:\n",
    "        print(f\"❌ Critical Error loading UMAP-reduced matrix: {e}.\")\n",
    "else:\n",
    "    print(f\"⚠️ UMAP-reduced matrix NOT FOUND at {tfidf_matrix_reduced_umap_path.resolve()}. Skipping UMAP-based clustering for now.\")\n",
    "\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Apply DBSCAN to SVD-reduced data ---\n",
    "if tfidf_matrix_reduced_svd is not None and tfidf_matrix_reduced_svd.shape[0] > 0:\n",
    "    print(\"\\n--- Applying DBSCAN to SVD-reduced data ---\")\n",
    "\n",
    "    # DBSCAN requires parameter tuning (eps and min_samples).\n",
    "    # Estimating eps using NearestNeighbors can be a starting point.\n",
    "    # min_samples is often set based on the number of dimensions (e.g., 2*n_dimensions).\n",
    "    min_samples_dbscan = max(2, 2 * tfidf_matrix_reduced_svd.shape[1]) # min_samples >= 2\n",
    "\n",
    "    print(f\"  Estimating eps for DBSCAN using NearestNeighbors (min_samples={min_samples_dbscan})...\")\n",
    "    # Using a sample for NearestNeighbors to speed up\n",
    "    sample_size_nn = min(10000, tfidf_matrix_reduced_svd.shape[0])\n",
    "    if tfidf_matrix_reduced_svd.shape[0] > sample_size_nn:\n",
    "        print(f\"  Using a sample of {sample_size_nn} for NearestNeighbors.\")\n",
    "        tfidf_matrix_sample_svd_nn, _ = resample(tfidf_matrix_reduced_svd, n_samples=sample_size_nn, replace=False, random_state=42)\n",
    "    else:\n",
    "        tfidf_matrix_sample_svd_nn = tfidf_matrix_reduced_svd\n",
    "\n",
    "\n",
    "    # Calculate the distance to the n_neighbors-th nearest neighbor\n",
    "    # min_samples is the number of neighbors (including the point itself)\n",
    "    # So we look at the distance to the (min_samples - 1)-th neighbor\n",
    "    if tfidf_matrix_sample_svd_nn.shape[0] > min_samples_dbscan:\n",
    "        try:\n",
    "            nn = NearestNeighbors(n_neighbors=min_samples_dbscan).fit(tfidf_matrix_sample_svd_nn)\n",
    "            distances, indices = nn.kneighbors(tfidf_matrix_sample_svd_nn)\n",
    "            # Sort distances and plot to find an elbow (visual method) or pick a value\n",
    "            # For automated selection, we might pick a percentile or look for a knee point.\n",
    "            # Let's pick a high percentile as a starting point for eps.\n",
    "            distances = np.sort(distances[:, min_samples_dbscan-1])\n",
    "            estimated_eps = np.percentile(distances, 90) # Example: 90th percentile\n",
    "\n",
    "            print(f\"  Estimated starting eps for DBSCAN: {estimated_eps:.4f}\")\n",
    "\n",
    "            # Now, evaluate DBSCAN for a range of eps values around the estimate\n",
    "            # We'll need to run DBSCAN and calculate metrics (Silhouette, Davies-Bouldin)\n",
    "            # This can be computationally expensive. Let's evaluate a few eps values.\n",
    "            eps_values_to_test = [estimated_eps * factor for factor in [0.5, 0.75, 1.0, 1.25, 1.5]] # Test around the estimate\n",
    "            # Ensure eps values are positive\n",
    "            eps_values_to_test = [eps for eps in eps_values_to_test if eps > 0]\n",
    "            print(f\"  Testing DBSCAN with min_samples={min_samples_dbscan} and eps values: {eps_values_to_test}\")\n",
    "\n",
    "            best_dbscan_score = -1.0 # Using Silhouette score for evaluation\n",
    "            best_dbscan_eps = None\n",
    "            best_dbscan_model = None\n",
    "            evaluated_dbscan_data = []\n",
    "\n",
    "            # Sample data for Silhouette/Davies-Bouldin calculation (speeds up computation)\n",
    "            sample_size_metrics = min(10000, tfidf_matrix_reduced_svd.shape[0])\n",
    "            if tfidf_matrix_reduced_svd.shape[0] > sample_size_metrics:\n",
    "                print(f\"  Sampling {sample_size_metrics} documents for DBSCAN metrics calculation.\")\n",
    "                tfidf_matrix_sample_svd_metrics, _ = resample(tfidf_matrix_reduced_svd, n_samples=sample_size_metrics, replace=False, random_state=42)\n",
    "            else:\n",
    "                tfidf_matrix_sample_svd_metrics = tfidf_matrix_reduced_svd\n",
    "\n",
    "\n",
    "            for eps in eps_values_to_test:\n",
    "                iteration_start_time = time.time()\n",
    "                print(f\"\\n--- Testing DBSCAN with eps = {eps:.4f}, min_samples = {min_samples_dbscan} --- (Starting at {time.ctime()})\")\n",
    "\n",
    "                # Apply DBSCAN to the full SVD-reduced data\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples_dbscan)\n",
    "                dbscan_labels = dbscan.fit_predict(tfidf_matrix_reduced_svd) # Fit on full data\n",
    "\n",
    "                n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "                n_noise_points = list(dbscan_labels).count(-1)\n",
    "\n",
    "                print(f\"  ✅ DBSCAN fitted. Found {n_clusters_dbscan} clusters and {n_noise_points} noise points.\")\n",
    "\n",
    "                current_silhouette = -1.0\n",
    "                current_davies_bouldin = float('inf')\n",
    "\n",
    "                # Calculate Silhouette and Davies-Bouldin Scores on sampled data\n",
    "                # Ensure sample size is sufficient and there's more than one cluster (excluding noise)\n",
    "                if n_clusters_dbscan >= 2 and tfidf_matrix_sample_svd_metrics.shape[0] >= n_clusters_dbscan:\n",
    "                    # Predict labels for sampled data using the fitted DBSCAN model\n",
    "                    # DBSCAN predict is not standard, usually you fit and get labels.\n",
    "                    # To evaluate on sample, we would ideally fit DBSCAN on the sample,\n",
    "                    # or use a metric that works with the full labels and data but is sample-based.\n",
    "                    # For simplicity here, let's calculate metrics on the full data if feasible,\n",
    "                    # or use a metric that handles noise points. Silhouette score can handle -1 labels.\n",
    "                    # Let's calculate metrics on the full data if it's not too memory intensive.\n",
    "                    # If full data is too large for metrics, we'd need a different approach or sample.\n",
    "\n",
    "                    # Let's calculate metrics on the full data if possible, otherwise use sample\n",
    "                    data_for_metrics = tfidf_matrix_reduced_svd\n",
    "                    labels_for_metrics = dbscan_labels\n",
    "\n",
    "                    # Ensure there are at least 2 unique labels (excluding -1) for metrics\n",
    "                    if len(set(labels_for_metrics) - {-1}) >= 2:\n",
    "                         # Silhouette score can handle noise points (-1)\n",
    "                         current_silhouette = silhouette_score(data_for_metrics, labels_for_metrics)\n",
    "                         # Davies-Bouldin Index does NOT handle noise points. We need to filter.\n",
    "                         # Filter out noise points for Davies-Bouldin\n",
    "                         non_noise_indices = np.where(labels_for_metrics != -1)[0]\n",
    "                         if len(np.unique(labels_for_metrics[non_noise_indices])) >= 2:\n",
    "                             current_davies_bouldin = davies_bouldin_score(data_for_metrics[non_noise_indices], labels_for_metrics[non_noise_indices])\n",
    "                         else:\n",
    "                             print(f\"  ⚠️ Skipping Davies-Bouldin for eps={eps:.4f}: Not enough non-noise clusters (>=2).\")\n",
    "                             current_davies_bouldin = float('inf') # Assign infinity if cannot compute\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(f\"  ⚠️ Skipping Silhouette/Davies-Bouldin for eps={eps:.4f}: Not enough clusters found (excluding noise).\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Skipping Silhouette/Davies-Bouldin for eps={eps:.4f}: Not enough clusters found ({n_clusters_dbscan}) or sample size too small.\")\n",
    "\n",
    "\n",
    "                silhouette_scores.append(current_silhouette)\n",
    "                davies_bouldin_scores.append(current_davies_bouldin)\n",
    "\n",
    "                print(f\"  ✅ Silhouette Score: {current_silhouette:.4f}\")\n",
    "                print(f\"  ✅ Davies-Bouldin Index: {current_davies_bouldin:.4f}\")\n",
    "\n",
    "                # Update best model based on Silhouette score (higher is better)\n",
    "                if current_silhouette > best_dbscan_score:\n",
    "                    best_dbscan_score = current_silhouette\n",
    "                    best_dbscan_eps = eps\n",
    "                    # Store parameters, not the fitted model if memory is an issue\n",
    "                    best_dbscan_model_params = {'eps': eps, 'min_samples': min_samples_dbscan}\n",
    "\n",
    "\n",
    "                evaluated_dbscan_data.append({\n",
    "                    'eps': eps,\n",
    "                    'min_samples': min_samples_dbscan,\n",
    "                    'n_clusters': n_clusters_dbscan,\n",
    "                    'n_noise': n_noise_points,\n",
    "                    'silhouette': current_silhouette,\n",
    "                    'davies_bouldin': current_davies_bouldin,\n",
    "                    # 'model': dbscan # Avoid storing large models if memory is tight\n",
    "                })\n",
    "\n",
    "                iteration_end_time = time.time()\n",
    "                elapsed_time = iteration_end_time - iteration_start_time\n",
    "                print(f\"--- DBSCAN (eps={eps:.4f}) processing complete in {elapsed_time:.2f} seconds ---\")\n",
    "                print(\"-\" * 20)\n",
    "\n",
    "            print(f\"\\n--- DBSCAN Optimal Parameter Selection Results (on SVD Data) ---\")\n",
    "            print(f\"Best eps based on max Silhouette Score ({best_dbscan_score:.4f}): {best_dbscan_eps:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # --- Step 4: Apply DBSCAN with Best Parameters and Save Labels ---\n",
    "            if best_dbscan_eps is not None:\n",
    "                print(f\"\\n--- Applying final DBSCAN with best parameters (eps={best_dbscan_eps:.4f}, min_samples={min_samples_dbscan}) ---\")\n",
    "                final_dbscan_model = DBSCAN(eps=best_dbscan_eps, min_samples=min_samples_dbscan)\n",
    "                final_dbscan_labels = final_dbscan_model.fit_predict(tfidf_matrix_reduced_svd)\n",
    "\n",
    "                # Save the DBSCAN cluster labels\n",
    "                dbscan_labels_svd_path = data_dir_path / f\"document_cluster_labels_dbscan_svd_eps{best_dbscan_eps:.4f}.joblib\"\n",
    "                # Clean up the filename to remove potentially invalid characters from float\n",
    "                dbscan_labels_svd_path_cleaned = data_dir_path / f\"document_cluster_labels_dbscan_svd_eps_{str(best_dbscan_eps).replace('.', '_')}.joblib\"\n",
    "\n",
    "                joblib.dump(final_dbscan_labels, dbscan_labels_svd_path_cleaned)\n",
    "                print(f\"✅ DBSCAN cluster labels saved to: {dbscan_labels_svd_path_cleaned.resolve()}\")\n",
    "\n",
    "                n_clusters_final = len(set(final_dbscan_labels)) - (1 if -1 in final_dbscan_labels else 0)\n",
    "                n_noise_final = list(final_dbscan_labels).count(-1)\n",
    "                print(f\"Final DBSCAN found {n_clusters_final} clusters and {n_noise_final} noise points.\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            else:\n",
    "                print(\"❌ Cannot apply final DBSCAN: No best eps found.\")\n",
    "\n",
    "\n",
    "        else:\n",
    "             print(f\"⚠️ Cannot estimate eps: Sample size ({tfidf_matrix_sample_svd_nn.shape[0]}) is not large enough compared to min_samples ({min_samples_dbscan}).\")\n",
    "             print(\"Consider reducing min_samples or using a smaller dataset.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred during DBSCAN processing: {e}\")\n",
    "        # Add more specific error handling if needed\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot proceed with DBSCAN: SVD-reduced matrix is not available.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
